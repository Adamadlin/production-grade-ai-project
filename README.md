# ğŸ§  Production-Grade AI Project  
### Local, Private, End-to-End RAG System (FastAPI + Ollama + Next.js)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green?logo=fastapi)
![Next.js](https://img.shields.io/badge/Next.js-Frontend-black?logo=next.js)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLMs-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

> âš¡ï¸ **A production-grade, fully local AI pipeline**  
> Retrieve â†’ Summarize â†’ Chat with your own data, **completely offline**.

---

## ğŸš€ Overview
This system is a **local Retrieval-Augmented Generation (RAG) stack** that runs end-to-end on your machine â€” no cloud, no API keys.

It combines:
- ğŸ§© **FastAPI backend** â€“ handles scraping, ingestion, embedding, and summarization  
- ğŸ–¥ **Next.js + Tailwind frontend** â€“ smooth chat-style interface  
- ğŸ§® **ChromaDB** â€“ vector database for retrieval  
- ğŸ¦™ **Ollama** â€“ local LLM runtime (e.g., `llama3:8b`, `qwen2.5:3b-instruct`)



---

## âš™ï¸ Quick Start

### Prerequisites
- macOS / Linux  
- Python 3.10 +  
- Node 18 + / npm  
- [Ollama](https://ollama.com) installed locally  



```bash
1ï¸âƒ£ Clone and install

git clone https://github.com/Adamadlin/production-grade-ai-project.git
cd production-grade-ai-project

python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt

cd frontend
npm install
cd ..

2ï¸âƒ£ Run the stack
Open three terminals:

# ğŸ§  Backend API
source .venv/bin/activate
make dev-backend

# ğŸ¤– Local models
make ollama

# ğŸ’¬ Frontend UI
cd frontend
npm run dev
Then visit â†’ http://localhost:3000

ğŸ§© How It Works

ğŸª„ 1. Ingest

Paste URLs into the â€œIngest URLsâ€ box â†’ the backend:
	1.	Fetches and scrapes pages
	2.	Cleans text
	3.	Chunks text (tokens size + overlap)
	4.	Embeds + stores in ChromaDB
	5.	Skips already indexed pages (unless force=true)

ğŸ” 2. Search

Performs semantic similarity search in ChromaDB, returning matching text chunks and their citations.

ğŸ§  3. Summarize

Retrieves â†’ builds prompt â†’ calls local LLM â†’ outputs a summary where every sentence ends with a citation.

Example query:

â€œHow to use Nmap on my local network?â€

Output:

The summary explains the tool usage and cites the Kali Linux pages used to build the answer.

â¸»

---

## ğŸ“„ Upload PDFs (New Feature â€“ Phase A)

You can now upload **local PDF files** directly to the backend â€” perfect for protected, paywalled, or offline documents.  
The system will extract text, chunk it, embed it, and add it to your vector collection automatically.

### Requirements
Make sure the following dependencies are installed (already in `requirements.txt`):
```bash
pip install python-multipart pdfminer.six

ğŸ§® UI Controls (Frontend)

ğŸ”§ Ingest Panel

controll       Description
--------       -----------   
URLs           One per line â€” pages to scrape + index
tokens         Approx. words per chunk (100â€“4000)
overlap        Words repeated between chunks
force          Re-index even if content unchanged
collection     Vector namespace (e.g. default, eula_docs)

Result Metrics: chunks, indexed, skipped, avg_chunk_words.



ğŸ’¬ Chat Panel

field      Purpose
----       -------
mode  ---   Search â†’ show chunks; Summarize â†’ generate answer
model  ---   Local LLM (llama3:8b, qwen2.5:3b-instruct, etc.)
k     ---      Number of chunks retrieved before answering
temperature ---   Creativity level (0.0 = strict facts)
max_tokens --- Limit for generated text
domain/source filters --- Restrict search to specific domains or paths
History sidebar --- Saved previous queries in localStorage


ğŸ”’ Privacy & Security
	â€¢	Everything runs locally via Ollama
	â€¢	No data leaves your machine
	â€¢	Outbound requests only when you fetch public URLs
	â€¢	Works offline once indexed
	â€¢	Ideal for private R&D, policy auditing, and training use


ğŸ— Architecture

Frontend (Next.js + Tailwind)
       â”‚
       â–¼
FastAPI Backend
 â”œâ”€â”€ /ingest        â†’ scrape â†’ clean â†’ chunk â†’ embed
 â”œâ”€â”€ /search        â†’ semantic retrieval from Chroma
 â”œâ”€â”€ /summarize     â†’ RAG prompt â†’ local LLM â†’ citations
 â”œâ”€â”€ /examples      â†’ example queries from registry
 â””â”€â”€ /health        â†’ system check
       â”‚
       â–¼
ChromaDB Vector Store
       â”‚
       â–¼
Ollama Runtime (local LLMs)

ğŸ¥ Demo

â–¶ Watch Demo on YouTube
https://youtu.be/lIFiL-V_m18  
*(Short demo: ingesting Kali Linux docs and asking security questions)*

Highlights
	â€¢	Ingests Kali Linux tool pages (Nmap, Metasploit, etc.)
	â€¢	Answers questions like â€œhow do I use Nmap?â€ with citations
	â€¢	Fully offline and reproducible
	â€¢	Uses llama3:8b and qwen2.5:3b-instruct



Layer       Technology
-----       ---------- 
Backend      FastAPI Â· Python Â· pydantic Â· httpx
Vector DB    ChromaDB
Frontend     Next.js 14 Â· TypeScript Â· Tailwind CSS
Models        Llama 3 Â· Qwen 2.5 (through Ollama)
Misc          SlowAPI (rate limiting) Â· Makefile (dev automation)


ğŸ§­ Repository Structure

production-grade-ai-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ clients/          # scraper + LLM clients
â”‚   â”œâ”€â”€ pipeline/         # ingest, clean, chunk, export
â”‚   â”œâ”€â”€ rag/              # vector DB + summarization
â”‚   â”œâ”€â”€ utils/            # hashing + registry helpers
â”‚   â”œâ”€â”€ config.py         # pydantic settings
â”‚   â””â”€â”€ main.py           # FastAPI entrypoint
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/app/page.tsx  # main UI
â”‚   â”œâ”€â”€ components/       # ChatBubble, Sidebar, etc.
â”‚   â””â”€â”€ lib/api.ts        # backend communication
â”‚
â”œâ”€â”€ Makefile              # quick dev commands
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt



ğŸ§° Development Commands

Command                 Description
------                  -----------
make dev-backend        Start FastAPI backend
make dev-frontend       Start Next.js frontend
make ollama             Serve Ollama + pull models
make seed               Ingest Wikipedia demo page
make fmt                Format Python code (ruff + black)
make test               Run pytest suite

ğŸ§‘â€ğŸ“ Author

Adam Adlin
ğŸš€ Full-stack developer & AI systems builder


â¸»

ğŸªª License

MIT License Â© 2025 Adam Adlin


ğŸŒŸ Contributing

Pull requests that improve usability, docs, or clarity are welcome.
If you build extensions or use it with your own data, share it via Issues or Discussions!





