# ğŸ§  Production-Grade AI Project  
**End-to-end Retrieval-Augmented Generation (RAG) system with FastAPI, Ollama, and Next.js**

---

## ğŸš€ Overview
A full-stack, production-ready **AI system** built completely from scratch.  
It combines a **Python FastAPI backend** for ingestion, retrieval, and summarization with a **Next.js + Tailwind** frontend that delivers a smooth, chat-style user experience.

ğŸ’¡ Paste any URL â€” Wikipedia, blogs, EULAs â€” and the system will:
- ğŸ” **Crawl** and clean it  
- ğŸ“š **Index** it in a local vector DB  
- ğŸ§  **Search** or **Summarize** with verified inline citations  
- âš¡ Run fully offline with local Ollama models  

---

## ğŸ§© Tech Stack

### ğŸ–¥ Backend (AI Core)
- **FastAPI** â€“ blazing-fast API framework  
- **Chroma Vector DB** â€“ persistent local embedding store  
- **Ollama** â€“ runs local LLMs (`llama3:8b`, `qwen2.5:3b-instruct`)  
- **Sentence Transformers** â€“ text embedding and retrieval  
- **SlowAPI** â€“ rate limiting middleware  
- **Structured Logging** â€“ clean, production-grade logs  

### ğŸ’» Frontend (UI)
- **Next.js 14 (App Router)** â€“ modern React framework  
- **React 18 + Hooks** â€“ declarative interface  
- **Tailwind CSS** â€“ clean, responsive design  
- **React Markdown + GFM** â€“ citation-friendly output  
- **TypeScript** â€“ safe and maintainable code  

---

## ğŸ—ï¸ Architecture
User â†’ Next.js UI â†’ FastAPI Backend â†’ RAG Pipeline â†’ Ollama (LLM)
â†“
Chroma Vector DB

### Flow:
1. `/ingest` â€” Scrape, clean, chunk, embed, and index documents.  
2. `/search` â€” Retrieve semantically similar text.  
3. `/summarize` â€” Generate a summary with strict citation rules.  

---

## ğŸ§± Folder Structure
PRODUCTION_GRADE_AI_PROJECT/
â”œâ”€â”€ app/                 # FastAPI backend
â”‚   â”œâ”€â”€ clients/         # Scraper, LLM, storage connectors
â”‚   â”œâ”€â”€ pipeline/        # Ingestion, cleaning, chunking, QC
â”‚   â”œâ”€â”€ rag/             # RAG + summarization logic
â”‚   â””â”€â”€ utils/           # Hashing, registry, timing, etc.
â”œâ”€â”€ frontend/            # Next.js + Tailwind app
â”‚   â”œâ”€â”€ src/app/         # Main pages
â”‚   â”œâ”€â”€ src/components/  # ChatBubble, Spinner, Toast
â”‚   â””â”€â”€ src/lib/         # API wrapper
â”œâ”€â”€ vectorstore/         # Persisted embeddings
â”œâ”€â”€ data/                # Registry / indexes
â”œâ”€â”€ out/                 # Exported datasets
â””â”€â”€ scripts/cli.py       # Optional CLI entr

---

## âš™ï¸ Local Setup

### 1ï¸âƒ£ Backend â€” FastAPI + Ollama
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Start Ollama (must have model pulled)
ollama serve
ollama pull llama3:8b

# Run the backend
uvicorn app.main:app --reload --port 8000

Test:
curl http://localhost:8000/health


2ï¸âƒ£ Frontend â€” Next.js UI
cd frontend
npm install
npm run dev
Now open ğŸ‘‰ http://localhost:3000

(Frontend automatically connects to http://localhost:8000 as defined in .env.local.)



ğŸ§  Example Usage
	1.	In the web UI, paste a URL (e.g. https://en.wikipedia.org/wiki/History_of_Microsoft)
	2.	Click Ingest to index it
	3.	Type a topic like â€œhistory of Microsoftâ€ and click Summarize
	4.	The result will show:
Microsoft was founded by Bill Gates and Paul Allen in 1975 (wiki:400â€“900).
Windows later dominated global PC markets (wiki:10800â€“11300).

ğŸ§° Environment Variables
.env (backend)
MODEL_NAME=llama3:8b
RATE_LIMIT_PER_MIN=30
ENV=local
OUT_DIR=out
DATA_DIR=data
VECTOR_DB_DIR=vectorstore

.env.local (frontend)
NEXT_PUBLIC_API_BASE=http://localhost:8000




ğŸ§ª API Reference
Endpoint :/health
Method:GET
Description:Check server status


Endpoint:/ingest
Method:POST
Description:Crawl and embed web 

Endpoint:/search
Method:GET
Description:Semantic retrieval


Endpoint:/summarize
Method:GET
Description:Generate a cited summary


