# ğŸ§  Production-Grade AI Project  
**End-to-End Retrieval-Augmented Generation (RAG) System built with FastAPI, Ollama, and Next.js**

---

## ğŸš€ Overview  
A **full-stack, production-ready AI system** built completely from scratch.  
It combines a **Python FastAPI backend** for ingestion, retrieval, and summarization with a **Next.js + Tailwind frontend** that delivers a smooth, chat-style experience.

> ğŸ’¡ **Paste any URL** â€” Wikipedia, blogs, EULAs â€” and the system will:  
> ğŸ” Crawl & clean it  
> ğŸ“š Index it in a local vector DB  
> ğŸ§  Search or summarize with verified inline citations  
> âš¡ Run fully offline with local Ollama models  

---

## ğŸ§© Tech Stack  

### ğŸ–¥ Backend (AI Core)
| Component | Purpose |
|------------|----------|
| **FastAPI** | High-performance async API framework |
| **Chroma Vector DB** | Persistent local embedding store |
| **Ollama** | Runs local LLMs *(llama3 8B / qwen2.5 3B-instruct)* |
| **Sentence Transformers** | Embedding & semantic retrieval |
| **SlowAPI** | Rate-limiting middleware |
| **Structured Logging** | Clean, production-grade logs |

### ğŸ’» Frontend (UI)
| Component | Purpose |
|------------|----------|
| **Next.js 14 (App Router)** | Modern React framework |
| **React 18 + Hooks** | Declarative interface |
| **Tailwind CSS** | Clean, responsive design |
| **React Markdown + GFM** | Citation-friendly output |
| **TypeScript** | Safe, maintainable code |

---

## ğŸ—ï¸ Architecture 
User
â†“
Next.js (Frontend)
â†“
FastAPI (Backend)
â†“
RAG Pipeline â†’ Ollama (LLM)
â†“
Chroma Vector DB


### Pipeline Flow
| Endpoint | Description |
|-----------|--------------|
| `/ingest` | Scrape â†’ clean â†’ chunk â†’ embed â†’ index documents |
| `/search` | Retrieve semantically similar text |
| `/summarize` | Generate summaries with strict citation rules |

---

## ğŸ§± Folder Structure 
â”œâ”€â”€ app/                # FastAPI backend
â”‚   â”œâ”€â”€ clients/        # Scraper, LLM, storage connectors
â”‚   â”œâ”€â”€ pipeline/       # Ingestion, cleaning, chunking, QC
â”‚   â”œâ”€â”€ rag/            # RAG + summarization logic
â”‚   â””â”€â”€ utils/          # Hashing, registry, timing, etc.
â”‚
â”œâ”€â”€ frontend/           # Next.js + Tailwind app
â”‚   â”œâ”€â”€ src/app/        # Main pages
â”‚   â”œâ”€â”€ src/components/ # ChatBubble, Spinner, Toast
â”‚   â””â”€â”€ src/lib/        # API wrapper
â”‚
â”œâ”€â”€ vectorstore/        # Persisted embeddings
â”œâ”€â”€ data/               # Registry / indexes
â”œâ”€â”€ out/                # Exported datasets
â””â”€â”€ scripts/cli.py      # Optional CLI entry

---

## âš™ï¸ Local Setup  

### 1ï¸âƒ£ Backend â€” FastAPI + Ollama
```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Start Ollama (ensure model pulled)
ollama serve
ollama pull llama3:8b

# Run the backend
uvicorn app.main:app --reload --port 8000

Test:
curl http://localhost:8000/health

2ï¸âƒ£ Frontend â€” Next.js UI
cd frontend
npm install
npm run dev

Then open ğŸ‘‰ http://localhost:3000
(The frontend automatically connects to http://localhost:8000 as defined in .env.local.)


Example Usage
	1.	In the web UI, paste a URL (e.g. https://en.wikipedia.org/wiki/History_of_Microsoft)
	2.	Click Ingest to index it
	3.	Type a topic like â€œhistory of Microsoftâ€ and click Summarize
	4.	The result will show citations, for example:

Microsoft was founded by Bill Gates and Paul Allen in 1975 (wiki:400â€“900).
Windows later dominated global PC markets (wiki:10800â€“11300).


Environment Variables
.env (Backend)

MODEL_NAME=llama3:8b
RATE_LIMIT_PER_MIN=30
ENV=local
OUT_DIR=out
DATA_DIR=data
VECTOR_DB_DIR=vectorstore
.env.local (Frontend)

NEXT_PUBLIC_API_BASE=http://localhost:8000



Endpoint :/health
Method :GET
Description : Check server status


Endpoint :/ingest
Method: POST
Descruption: Crawl + embed web data


Endpoint :/search
Method: GET
Description :Semantic retrieval

Endpoint : /summarize
Method: GET
Description :Generate a cited summary



License

MIT Â© 2025 Adam Adlin


If you like this project, please â­ star the repo â€” it really helps!
Connect with me on LinkedIn for updates and future releases.
